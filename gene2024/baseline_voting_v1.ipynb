{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 암환자 유전체 데이터 기반 암종 분류 AI 모델 개발\n",
    "\n",
    "\n",
    "- '2024 생명연구자원 AI활용 경진대회'는 바이오 데이터를 기반으로 한 AI 기술의 문제 해결 능력을 탐구하는 것을 목표로 합니다. <br>이 대회는 바이오 분야에서 AI 활용의 저변을 확대하고, 복잡한 바이오 데이터를 효율적으로 분석 및 해석할 수 있는 AI 알고리즘 개발에 초점을 맞추고 있습니다. <br><br>\n",
    "- 본 대회의 구체적인 과제는 암환자 유전체 데이터의 변이 정보를 활용하여 암종을 분류하는 AI 모델을 개발하는 것입니다. <br>참가자들은 제공된 학습 데이터셋(암환자 유전체 변이 정보)을 사용하여 특정 변이 정보를 바탕으로 암종을 정확하게 분류할 수 있는 AI 알고리즘을 개발해야 합니다. <br><br>\n",
    "- 이 대회의 궁극적인 목적은 바이오 데이터의 활용도를 높이고, 바이오 분야에서 AI 기술의 적용 가능성을 극대화하며, 인공지능 기술이 실제 바이오 의료 문제 해결에 어떻게 기여할 수 있는지 탐구하는 것입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, OrdinalEncoder\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(r\".\\train.csv\")\n",
    "test = pd.read_csv(r\".\\test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원래 레이블: ACC, 변환된 숫자: 0\n",
      "원래 레이블: BLCA, 변환된 숫자: 1\n",
      "원래 레이블: BRCA, 변환된 숫자: 2\n",
      "원래 레이블: CESC, 변환된 숫자: 3\n",
      "원래 레이블: COAD, 변환된 숫자: 4\n",
      "원래 레이블: DLBC, 변환된 숫자: 5\n",
      "원래 레이블: GBMLGG, 변환된 숫자: 6\n",
      "원래 레이블: HNSC, 변환된 숫자: 7\n",
      "원래 레이블: KIPAN, 변환된 숫자: 8\n",
      "원래 레이블: KIRC, 변환된 숫자: 9\n",
      "원래 레이블: LAML, 변환된 숫자: 10\n",
      "원래 레이블: LGG, 변환된 숫자: 11\n",
      "원래 레이블: LIHC, 변환된 숫자: 12\n",
      "원래 레이블: LUAD, 변환된 숫자: 13\n",
      "원래 레이블: LUSC, 변환된 숫자: 14\n",
      "원래 레이블: OV, 변환된 숫자: 15\n",
      "원래 레이블: PAAD, 변환된 숫자: 16\n",
      "원래 레이블: PCPG, 변환된 숫자: 17\n",
      "원래 레이블: PRAD, 변환된 숫자: 18\n",
      "원래 레이블: SARC, 변환된 숫자: 19\n",
      "원래 레이블: SKCM, 변환된 숫자: 20\n",
      "원래 레이블: STES, 변환된 숫자: 21\n",
      "원래 레이블: TGCT, 변환된 숫자: 22\n",
      "원래 레이블: THCA, 변환된 숫자: 23\n",
      "원래 레이블: THYM, 변환된 숫자: 24\n",
      "원래 레이블: UCEC, 변환된 숫자: 25\n"
     ]
    }
   ],
   "source": [
    "# SUBCLASS 가 범주형이기 때문에 LabelEncoder 사용\n",
    "le_subclass = LabelEncoder()\n",
    "train['SUBCLASS'] = le_subclass.fit_transform(train['SUBCLASS'])\n",
    "\n",
    "# 변환된 레이블 확인\n",
    "for i, label in enumerate(le_subclass.classes_):\n",
    "    print(f\"원래 레이블: {label}, 변환된 숫자: {i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# 아미노산 코드와 그들의 성질을 매핑\n",
    "amino_acid_properties = {\n",
    "    'A': 'nonpolar',    # Alanine\n",
    "    'R': 'positive',    # Arginine\n",
    "    'N': 'polar',       # Asparagine\n",
    "    'D': 'negative',    # Aspartic Acid\n",
    "    'C': 'polar',       # Cysteine\n",
    "    'Q': 'polar',       # Glutamine\n",
    "    'E': 'negative',    # Glutamic Acid\n",
    "    'G': 'nonpolar',    # Glycine\n",
    "    'H': 'positive',    # Histidine\n",
    "    'I': 'nonpolar',    # Isoleucine\n",
    "    'L': 'nonpolar',    # Leucine\n",
    "    'K': 'positive',    # Lysine\n",
    "    'M': 'nonpolar',    # Methionine\n",
    "    'F': 'aromatic',    # Phenylalanine\n",
    "    'P': 'nonpolar',    # Proline\n",
    "    'S': 'polar',       # Serine\n",
    "    'T': 'polar',       # Threonine\n",
    "    'W': 'aromatic',    # Tryptophan\n",
    "    'Y': 'aromatic',    # Tyrosine\n",
    "    'V': 'nonpolar',    # Valine\n",
    "}\n",
    "\n",
    "# 변이 유형을 분류하는 함수 정의\n",
    "def classify_mutation(mutation):\n",
    "    # 결측치 처리\n",
    "    if pd.isnull(mutation):\n",
    "        return None  # 또는 특정 코드로 지정 가능\n",
    "\n",
    "    mutation = str(mutation).strip()\n",
    "\n",
    "    # WT 체크\n",
    "    if mutation == 'WT':\n",
    "        return 0  # WT (Wild Type)\n",
    "\n",
    "    # 프레임시프트 돌연변이 체크 ('fs' 포함)\n",
    "    if 'fs' in mutation:\n",
    "        return 5  # 프레임시프트 돌연변이\n",
    "\n",
    "    # 중단 돌연변이 체크 ('*' 포함)\n",
    "    if '*' in mutation:\n",
    "        return 4  # 중단 돌연변이\n",
    "\n",
    "    # 돌연변이 패턴 매칭 (예: 'R496Q', 'L1700L')\n",
    "    match = re.match(r'^([A-Z])(\\d+)([A-Z])$', mutation)\n",
    "    if match:\n",
    "        from_aa = match.group(1)  # 원래 아미노산\n",
    "        position = match.group(2) # 위치 (사용하지 않음)\n",
    "        to_aa = match.group(3)    # 변이된 아미노산\n",
    "\n",
    "        # 침묵 돌연변이 체크 (아미노산이 동일한 경우)\n",
    "        if from_aa == to_aa:\n",
    "            return 1  # 침묵 돌연변이\n",
    "\n",
    "        # 아미노산 성질 가져오기\n",
    "        from_property = amino_acid_properties.get(from_aa)\n",
    "        to_property = amino_acid_properties.get(to_aa)\n",
    "\n",
    "        # 아미노산 코드가 유효한지 확인\n",
    "        if from_property is None or to_property is None:\n",
    "            return 6  # 알 수 없는 아미노산 코드\n",
    "\n",
    "        # 보존적 돌연변이 체크 (아미노산 성질이 동일한 경우)\n",
    "        if from_property == to_property:\n",
    "            return 2  # 보존적 돌연변이\n",
    "        else:\n",
    "            return 3  # 비보존적 돌연변이\n",
    "    else:\n",
    "        # 패턴 매칭 실패한 경우\n",
    "        return 6  # 매칭 실패한 경우 6 반환\n",
    "\n",
    "# 다중 치환을 처리하는 함수 정의\n",
    "def classify_multiple_mutations(mutation_string):\n",
    "    # 결측치 처리\n",
    "    if pd.isnull(mutation_string):\n",
    "        return None  # 또는 특정 코드로 지정 가능\n",
    "\n",
    "    # 변이 문자열을 공백으로 분리\n",
    "    mutations = str(mutation_string).strip().split()\n",
    "\n",
    "    labels = []\n",
    "    for mutation in mutations:\n",
    "        label = classify_mutation(mutation)\n",
    "        if label is not None:\n",
    "            labels.append(label)\n",
    "    if labels:\n",
    "        # 가장 높은 값을 반환\n",
    "        return max(labels)\n",
    "    else:\n",
    "        return None  # 또는 특정 코드로 지정 가능\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 제외할 열 목록 (예시로 'ID'와 'SUBCLASS'를 제외)\n",
    "exclude_cols = ['ID', 'SUBCLASS']\n",
    "\n",
    "# 변이 데이터가 있는 열 목록\n",
    "mutation_cols = [col for col in train.columns if col not in exclude_cols]\n",
    "\n",
    "# 각 열에 함수 적용\n",
    "for col in mutation_cols:\n",
    "    train[col] = train[col].apply(classify_multiple_mutations)\n",
    "\n",
    "# 결과 출력 (일부 열만 표시)\n",
    "# print(train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(train.head(19))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 제외할 열 목록 (예시로 'ID'와 'SUBCLASS'를 제외)\n",
    "exclude_cols = ['ID', 'SUBCLASS']\n",
    "\n",
    "# 변이 데이터가 있는 열 목록\n",
    "mutation_cols = [col for col in train.columns if col not in exclude_cols]\n",
    "\n",
    "# 각 열에 함수 적용\n",
    "for col in mutation_cols:\n",
    "    test[col] = test[col].apply(classify_multiple_mutations)\n",
    "\n",
    "# 결과 출력 (일부 열만 표시)\n",
    "# print(test.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Define and Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LGBM 최적화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.metrics import accuracy_score, classification_report\n",
    "# # import lightgbm as lgb\n",
    "# import numpy as np\n",
    "# from itertools import product\n",
    "# from imblearn.over_sampling import SMOTE\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.svm import SVC\n",
    "# from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "# from lightgbm import LGBMClassifier\n",
    "\n",
    "# voting_clf = VotingClassifier(\n",
    "#     estimators=[\n",
    "#         ('lr',LogisticRegression()),\n",
    "#         ('rf',RandomForestClassifier()),\n",
    "#         ('svc',SVC()),\n",
    "#         ('lgb',LGBMClassifier())\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "# # 1. 데이터 준비\n",
    "# # 타겟: 'SUBCLASS', 특징: 'SUBCLASS'와 'ID'를 제외한 나머지 열\n",
    "# X = train.drop(columns=['SUBCLASS', 'ID'])  # 특징 데이터 (SUBCLASS를 제외한 모든 열)\n",
    "# y = train['SUBCLASS']  # 타겟 데이터 (SUBCLASS)\n",
    "\n",
    "# # 2. 학습 세트와 테스트 세트로 데이터 나누기\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# ss = StandardScaler()\n",
    "# x_train_scaled = ss.fit_transform(X_train)\n",
    "# x_test_scaled =  ss.fit_transform(X_test)\n",
    "\n",
    "# # Apply SMOTE to balance the training data\n",
    "# smote = SMOTE(random_state=42)\n",
    "# X_train_resampled, y_train_resampled = smote.fit_resample(x_train_scaled, y_train)\n",
    "\n",
    "# # Define a set of hyperparameters to iterate over for tuning\n",
    "# learning_rates = [0.001, 0.05, 0.01, 0.1]\n",
    "# num_leaves_options = [5, 10, 15]\n",
    "# max_depths = [-1, 5, 10]  # -1 indicates no limit\n",
    "# boosting_types = ['gbdt']  # 'dart', 'goss'\n",
    "\n",
    "# # Create a list of all combinations of hyperparameters\n",
    "# tuning_params = list(product(learning_rates, num_leaves_options, max_depths, boosting_types))\n",
    "\n",
    "# best_train_accuracy = 0\n",
    "# best_test_accuracy = 0\n",
    "# best_params = None\n",
    "\n",
    "# for lr, num_leaves, max_depth, boosting_type in tuning_params:\n",
    "#     # Set parameters for the current iteration\n",
    "#     params = {\n",
    "#         'objective': 'multiclass',\n",
    "#         'metric': 'multi_logloss',\n",
    "#         'boosting_type': boosting_type,\n",
    "#         'learning_rate': lr,\n",
    "#         'num_leaves': num_leaves,\n",
    "#         'max_depth': max_depth,\n",
    "#         'verbose': -1,\n",
    "#         'num_class': len(y.unique())\n",
    "#     }\n",
    "    \n",
    "#     # Create and train the LightGBM model using LGBMClassifier\n",
    "#     model = LGBMClassifier(**params)\n",
    "#     model.fit(X_train_resampled, y_train_resampled)\n",
    "    \n",
    "#     # Make predictions on the train and test data\n",
    "#     y_train_pred = model.predict(X_train_resampled)\n",
    "#     y_test_pred = model.predict(X_test)\n",
    "    \n",
    "#     # Calculate accuracy\n",
    "#     train_accuracy = accuracy_score(y_train_resampled, y_train_pred)\n",
    "#     test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "    \n",
    "#     # Check if this is the best model so far\n",
    "#     if test_accuracy > best_test_accuracy:\n",
    "#         best_train_accuracy = train_accuracy\n",
    "#         best_test_accuracy = test_accuracy\n",
    "#         best_params = (lr, num_leaves, max_depth, boosting_type)\n",
    "#         best_model = model  # Save the best model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Voting\n",
    "    - logisticregression\n",
    "    - randomforestclassifier\n",
    "    - svm.SVC\n",
    "    - lightgbm\n",
    "    - xgboost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.metrics import accuracy_score, classification_report\n",
    "# from sklearn.ensemble import VotingClassifier\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.svm import SVC\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# import lightgbm as lgb\n",
    "# import xgboost as xgb\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# # 1. 데이터 준비\n",
    "# # 타겟: 'SUBCLASS', 특징: 'SUBCLASS'와 'ID'를 제외한 나머지 열\n",
    "# X = train.drop(columns=['SUBCLASS', 'ID'])  # 특징 데이터 (SUBCLASS를 제외한 모든 열)\n",
    "# y = train['SUBCLASS']  # 타겟 데이터 (SUBCLASS)\n",
    "\n",
    "# # 2. 학습 세트와 테스트 세트로 데이터 나누기\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "# # 3. 데이터 스케일링\n",
    "# ss = StandardScaler()\n",
    "# X_train_scaled = ss.fit_transform(X_train)\n",
    "# X_test_scaled = ss.transform(X_test)  # Use transform instead of fit_transform on test set\n",
    "\n",
    "# # 4. SMOTE 적용하여 학습 데이터 균형 맞추기\n",
    "# smote = SMOTE(random_state=42)\n",
    "# X_train_resampled, y_train_resampled = smote.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# # 5. 개별 모델 정의\n",
    "# models = [\n",
    "#     ('lightgbm', lgb.LGBMClassifier(\n",
    "#         objective='multiclass',\n",
    "#         metric='multi_logloss',\n",
    "#         boosting_type='gbdt',\n",
    "#         learning_rate=0.1,\n",
    "#         num_leaves=15,\n",
    "#         max_depth=10,\n",
    "#         verbose=-1,\n",
    "#         num_class=len(y.unique())\n",
    "#     )),\n",
    "#     ('xgboost', xgb.XGBClassifier(\n",
    "#         objective='multi:softprob',\n",
    "#         eval_metric='mlogloss',\n",
    "#         use_label_encoder=False,\n",
    "#         learning_rate=0.1,\n",
    "#         max_depth=10,\n",
    "#         n_estimators=100\n",
    "#     )),\n",
    "#     ('logistic', LogisticRegression(max_iter=1000)),\n",
    "#     ('svm', SVC(probability=True)),  # Enable probability estimates for voting\n",
    "#     ('random_forest', RandomForestClassifier(n_estimators=100))\n",
    "# ]\n",
    "\n",
    "# # 6. 각 모델 학습\n",
    "# for name, model in tqdm(models, desc=\"Training Models\"):\n",
    "#     model.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# # 7. Voting Classifier 정의 (앙상블)\n",
    "# voting_clf = VotingClassifier(\n",
    "#     estimators=models,\n",
    "#     voting='soft'  # Use soft voting to consider probabilities of each class\n",
    "# )\n",
    "\n",
    "# # 8. Voting Classifier 학습 및 평가\n",
    "# voting_clf.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# # 예측 및 평가\n",
    "# y_pred = voting_clf.predict(X_test_scaled)\n",
    "# print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "# print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Print the best results and parameters\n",
    "# print(f\"Best Train Accuracy: {best_train_accuracy * 100:.2f}%\")\n",
    "# print(f\"Best Test Accuracy: {best_test_accuracy * 100:.2f}%\")\n",
    "# print(f\"Best Parameters: Learning Rate={best_params[0]}, Num Leaves={best_params[1]}, Max Depth={best_params[2]}, Boosting Type={best_params[3]}\")\n",
    "\n",
    "# # Generate and print classification report for the best model on test data\n",
    "# print(\"\\nClassification Report:\")\n",
    "# print(classification_report(y_test, y_test_pred))\n",
    "\n",
    "# # Use the best model to make predictions on the test set and store them\n",
    "# predictions = best_model.predict(X_test)\n",
    "\n",
    "# # Print or save predictions as needed\n",
    "# print(\"Predictions on Test Set:\")\n",
    "# print(predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델 최적화 후 voting 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hyperparameter Tuning: 100%|██████████| 36/36 [20:04<00:00, 33.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Train Accuracy: 87.79%\n",
      "Best Test Accuracy: 34.57%\n",
      "Best Parameters: Learning Rate=0.05, Num Leaves=10, Max Depth=-1, Boosting Type=gbdt\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.64      0.75        14\n",
      "           1       0.25      0.05      0.08        21\n",
      "           2       0.52      0.48      0.50       157\n",
      "           3       0.19      0.10      0.13        31\n",
      "           4       0.78      0.62      0.69        45\n",
      "           5       1.00      0.14      0.25         7\n",
      "           6       0.29      0.26      0.28        92\n",
      "           7       0.44      0.36      0.40        45\n",
      "           8       0.11      0.12      0.11       103\n",
      "           9       0.13      0.16      0.14        67\n",
      "          10       0.50      0.59      0.54        32\n",
      "          11       0.17      0.17      0.17        46\n",
      "          12       0.54      0.45      0.49        31\n",
      "          13       0.12      0.03      0.04        37\n",
      "          14       0.18      0.08      0.11        36\n",
      "          15       0.20      0.41      0.27        51\n",
      "          16       0.14      0.04      0.06        24\n",
      "          17       0.15      0.17      0.16        29\n",
      "          18       0.26      0.26      0.26        53\n",
      "          19       0.14      0.12      0.13        40\n",
      "          20       0.66      0.49      0.56        55\n",
      "          21       0.33      0.26      0.29        76\n",
      "          22       0.38      0.20      0.26        25\n",
      "          23       0.61      0.63      0.62        65\n",
      "          24       0.07      0.53      0.13        19\n",
      "          25       0.80      0.50      0.62        40\n",
      "\n",
      "    accuracy                           0.32      1241\n",
      "   macro avg       0.38      0.30      0.31      1241\n",
      "weighted avg       0.36      0.32      0.33      1241\n",
      "\n",
      "Predictions on Test Set (LightGBM):\n",
      "[23 19  2 ...  6 11  7]\n",
      "Accuracy (Voting Classifier): 0.29814665592264306\n",
      "Classification Report (Voting Classifier):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.57      0.70        14\n",
      "           1       0.40      0.10      0.15        21\n",
      "           2       0.55      0.47      0.51       157\n",
      "           3       0.10      0.06      0.08        31\n",
      "           4       0.74      0.64      0.69        45\n",
      "           5       0.50      0.14      0.22         7\n",
      "           6       0.23      0.21      0.22        92\n",
      "           7       0.31      0.22      0.26        45\n",
      "           8       0.09      0.08      0.08       103\n",
      "           9       0.12      0.15      0.13        67\n",
      "          10       0.35      0.62      0.45        32\n",
      "          11       0.16      0.20      0.18        46\n",
      "          12       0.29      0.13      0.18        31\n",
      "          13       0.00      0.00      0.00        37\n",
      "          14       0.21      0.17      0.19        36\n",
      "          15       0.22      0.37      0.27        51\n",
      "          16       0.14      0.08      0.11        24\n",
      "          17       0.11      0.24      0.16        29\n",
      "          18       0.22      0.23      0.22        53\n",
      "          19       0.12      0.15      0.14        40\n",
      "          20       0.70      0.55      0.61        55\n",
      "          21       0.31      0.28      0.29        76\n",
      "          22       0.25      0.04      0.07        25\n",
      "          23       0.56      0.65      0.60        65\n",
      "          24       0.09      0.47      0.15        19\n",
      "          25       0.61      0.47      0.54        40\n",
      "\n",
      "    accuracy                           0.30      1241\n",
      "   macro avg       0.32      0.28      0.28      1241\n",
      "weighted avg       0.32      0.30      0.30      1241\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from itertools import product\n",
    "from tqdm import tqdm\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# 1. 데이터 준비\n",
    "# 타겟: 'SUBCLASS', 특징: 'SUBCLASS'와 'ID'를 제외한 나머지 열\n",
    "X = train.drop(columns=['SUBCLASS', 'ID'])  # 특징 데이터 (SUBCLASS를 제외한 모든 열)\n",
    "y = train['SUBCLASS']  # 타겟 데이터 (SUBCLASS)\n",
    "\n",
    "# 2. 학습 세트와 테스트 세트로 데이터 나누기\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "# 3. 데이터 스케일링\n",
    "ss = StandardScaler()\n",
    "X_train_scaled = ss.fit_transform(X_train)\n",
    "X_test_scaled = ss.transform(X_test)  # Use transform instead of fit_transform on test set\n",
    "\n",
    "# 4. SMOTE 적용하여 학습 데이터 균형 맞추기\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "# Hyperparameter tuning for LightGBM with parallel processing\n",
    "learning_rates = [0.001, 0.05, 0.01, 0.1]\n",
    "num_leaves_options = [5, 10, 15]\n",
    "max_depths = [-1, 5, 10]  # -1 indicates no limit\n",
    "boosting_types = ['gbdt']  # 'dart', 'goss'\n",
    "\n",
    "tuning_params = list(product(learning_rates, num_leaves_options, max_depths, boosting_types))\n",
    "\n",
    "best_train_accuracy = 0\n",
    "best_test_accuracy = 0\n",
    "best_params = None\n",
    "\n",
    "for lr, num_leaves, max_depth, boosting_type in tqdm(tuning_params, desc=\"Hyperparameter Tuning\"):\n",
    "    params = {\n",
    "        'objective': 'multiclass',\n",
    "        'metric': 'multi_logloss',\n",
    "        'boosting_type': boosting_type,\n",
    "        'learning_rate': lr,\n",
    "        'num_leaves': num_leaves,\n",
    "        'max_depth': max_depth,\n",
    "        'verbose': -1,\n",
    "        'num_class': len(y.unique()),\n",
    "        'n_jobs': -1  # Use all available cores for LightGBM training\n",
    "    }\n",
    "    \n",
    "    model = lgb.LGBMClassifier(**params)\n",
    "    model.fit(X_train_resampled, y_train_resampled)\n",
    "    \n",
    "    y_train_pred = model.predict(X_train_resampled)\n",
    "    y_test_pred = model.predict(X_test_scaled)\n",
    "    \n",
    "    train_accuracy = accuracy_score(y_train_resampled, y_train_pred)\n",
    "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "    \n",
    "    if test_accuracy > best_test_accuracy:\n",
    "        best_train_accuracy = train_accuracy\n",
    "        best_test_accuracy = test_accuracy\n",
    "        best_params = (lr, num_leaves, max_depth, boosting_type)\n",
    "        best_model = model\n",
    "\n",
    "# Print the best results and parameters from hyperparameter tuning\n",
    "print(f\"Best Train Accuracy: {best_train_accuracy * 100:.2f}%\")\n",
    "print(f\"Best Test Accuracy: {best_test_accuracy * 100:.2f}%\")\n",
    "print(f\"Best Parameters: Learning Rate={best_params[0]}, Num Leaves={best_params[1]}, Max Depth={best_params[2]}, Boosting Type={best_params[3]}\")\n",
    "\n",
    "# Generate and print classification report for the best LightGBM model on test data\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_test_pred))\n",
    "\n",
    "# Use the best LightGBM model to make predictions on the test set and store them\n",
    "predictions_lgbm = best_model.predict(X_test_scaled)\n",
    "\n",
    "# Print predictions from the best LightGBM model\n",
    "print(\"Predictions on Test Set (LightGBM):\")\n",
    "print(predictions_lgbm)\n",
    "\n",
    "# Define the CatBoost model with maximum CPU usage\n",
    "catboost_model = CatBoostClassifier(\n",
    "    iterations=100,\n",
    "    learning_rate=0.1,\n",
    "    depth=10,\n",
    "    loss_function='MultiClass',\n",
    "    verbose=0,\n",
    "    thread_count=-1  # Use all available cores for CatBoost training\n",
    ")\n",
    "\n",
    "# Ensemble Voting Classifier with the best LightGBM model included and parallel processing enabled\n",
    "models = [\n",
    "    ('lightgbm', best_model),  # Use the tuned LightGBM model here\n",
    "    ('xgboost', xgb.XGBClassifier(\n",
    "        objective='multi:softprob',\n",
    "        eval_metric='mlogloss',\n",
    "        use_label_encoder=False,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=10,\n",
    "        n_estimators=100,\n",
    "        n_jobs=-1)),  # Use all available cores for XGBoost training\n",
    "    ('logistic', LogisticRegression(max_iter=1000)),\n",
    "    ('svm', SVC(probability=True)),  # Enable probability estimates for voting; SVM does not support n_jobs directly in scikit-learn.\n",
    "    ('random_forest', RandomForestClassifier(n_estimators=100, n_jobs=-1)),  # Use all available cores for Random Forest training\n",
    "    ('catboost', catboost_model)  # Add the CatBoost model with maximum CPU usage enabled.\n",
    "]\n",
    "\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=models,\n",
    "    voting='soft',  # Use soft voting to consider probabilities of each class.\n",
    "    n_jobs=-1       # Use all available cores for Voting Classifier fitting.\n",
    ")\n",
    "\n",
    "# Fit the Voting Classifier using all available CPU cores.\n",
    "voting_clf.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Make predictions and evaluate the Voting Classifier.\n",
    "y_pred_voting = voting_clf.predict(X_test_scaled)\n",
    "print(\"Accuracy (Voting Classifier):\", accuracy_score(y_test, y_pred_voting))\n",
    "print(\"Classification Report (Voting Classifier):\\n\", classification_report(y_test, y_pred_voting))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from catboost import CatBoostClassifier\n",
    "\n",
    "# # Define the CatBoost model\n",
    "# catboost_model = CatBoostClassifier(\n",
    "#     iterations=100,\n",
    "#     learning_rate=0.1,\n",
    "#     depth=10,\n",
    "#     loss_function='MultiClass',\n",
    "#     verbose=0  # Set verbose to 0 to suppress output during fitting\n",
    "# )\n",
    "\n",
    "# # Add CatBoost to the list of models for the Voting Classifier\n",
    "# models = [\n",
    "#     ('lightgbm', best_model),  # Use the tuned LightGBM model here\n",
    "#     ('xgboost', xgb.XGBClassifier(\n",
    "#         objective='multi:softprob',\n",
    "#         eval_metric='mlogloss',\n",
    "#         use_label_encoder=False,\n",
    "#         learning_rate=0.1,\n",
    "#         max_depth=10,\n",
    "#         n_estimators=100)),\n",
    "#     ('logistic', LogisticRegression(max_iter=1000)),\n",
    "#     ('svm', SVC(probability=True)),  # Enable probability estimates for voting\n",
    "#     ('random_forest', RandomForestClassifier(n_estimators=100)),\n",
    "#     ('catboost', catboost_model)  # Add the CatBoost model\n",
    "# ]\n",
    "\n",
    "# voting_clf = VotingClassifier(\n",
    "#     estimators=models,\n",
    "#     voting='soft'  # Use soft voting to consider probabilities of each class\n",
    "# )\n",
    "\n",
    "# # Fit the Voting Classifier\n",
    "# voting_clf.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# # Make predictions and evaluate the Voting Classifier\n",
    "# y_pred_voting = voting_clf.predict(X_test_scaled)\n",
    "# print(\"Accuracy (Voting Classifier):\", accuracy_score(y_test, y_pred_voting))\n",
    "# print(\"Classification Report (Voting Classifier):\\n\", classification_report(y_test, y_pred_voting))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.drop(columns=['ID'])\n",
    "# X_encoded = test_X.copy()\n",
    "# X_encoded[categorical_columns] = ordinal_encoder.transform(test_X[categorical_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([20, 25, 25, ..., 25, 25, 25])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_labels = le_subclass.inverse_transform(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submisson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "submisson = pd.read_csv(\"./sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "submisson[\"SUBCLASS\"] = original_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "submisson.to_csv('./baseline_submission.csv', encoding='UTF-8-sig', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
